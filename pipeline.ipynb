{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KEEP IN MIND:\n",
    "- before submission train on entire train and val\n",
    "- how deal with test\n",
    "- clean up code and everything to submit\n",
    "- make sure to zip properly for submission\n",
    "- are all important columns included"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import lightgbm as lgb\n",
    "from scipy.stats import spearmanr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions used\n",
    "\n",
    "\n",
    "# extends data with simple features:\n",
    "# > convert strand from + - to 1 0\n",
    "# > gene length\n",
    "# > transcription site length\n",
    "# > ratio of transciption site length to gene length\n",
    "def extend_df(df, cols_to_keep):\n",
    "    # Add a binary column for strand\n",
    "    df['strand_binary'] = df['strand'].map({'+': 1, '-': 0})\n",
    "    \n",
    "    # gene length\n",
    "    df['gene_length'] = df['gene_end'] - df['gene_start']\n",
    "    \n",
    "    # transcription site length\n",
    "    df['trans_site_len'] = df['TSS_end'] - df['TSS_start']\n",
    "    \n",
    "    # ratio transcription site length & land gene length\n",
    "    df['trans_gene_ratio'] = df['trans_site_len'] / df['gene_length']\n",
    "\n",
    "    new_cols = ['strand_binary', 'gene_length', 'trans_site_len', 'trans_gene_ratio']\n",
    "\n",
    "    \n",
    "    return df[cols_to_keep + new_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load already feature engineered data\n",
    "\n",
    "\n",
    "# Paths\n",
    "X_1_train_path = 'data/X1-train/features.tsv'\n",
    "y_1_train_path = 'data/X1-train/y.tsv'\n",
    "\n",
    "X_1_val_path = 'data/X1-val/features.tsv'\n",
    "y_1_val_path = 'data/X1-val/y.tsv'\n",
    "\n",
    "X_2_train_path = 'data/X2-train/features.tsv'\n",
    "y_2_train_path = 'data/X2-train/y.tsv'\n",
    "\n",
    "X_2_val_path = 'data/X2-val/features.tsv'\n",
    "y_2_val_path = 'data/X2-val/y.tsv'\n",
    "\n",
    "# Load data\n",
    "X_1_train = pd.read_csv(X_1_train_path, sep='\\t')\n",
    "y_1_train = pd.read_csv(y_1_train_path, sep='\\t')\n",
    "\n",
    "X_1_val = pd.read_csv(X_1_val_path, sep='\\t')\n",
    "y_1_val = pd.read_csv(y_1_val_path, sep='\\t')\n",
    "\n",
    "X_2_train = pd.read_csv(X_2_train_path, sep='\\t')\n",
    "y_2_train = pd.read_csv(y_2_train_path, sep='\\t')\n",
    "\n",
    "X_2_val = pd.read_csv(X_2_val_path, sep='\\t')\n",
    "y_2_val = pd.read_csv(y_2_val_path, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some further data preprocessing\n",
    "\n",
    "\n",
    "cols_to_keep = ['DNase_num_peaks', 'DNase_avg_peaks', 'H3K4me1_num_peaks', 'H3K4me1_avg_peaks', 'H3K4me3_num_peaks', \n",
    "                'H3K4me3_avg_peaks', 'H3K27ac_num_peaks', 'H3K27ac_avg_peaks']\n",
    "\n",
    "# cols_to_keep = ['dnase_val', 'dnase_dist', 'H3K4me3_signal', 'H3K4me3_distance', 'H3K4me3_num_peaks', \n",
    "#                 'H3K4me3_avg_peaks', 'H3K27ac_signal', 'H3K27ac_distance', 'H3K27ac_num_peaks', \n",
    "#                 'H3K27ac_avg_peaks']\n",
    "\n",
    "\n",
    "# extend data by some basic transcription site features\n",
    "X_1_train = extend_df(X_1_train, cols_to_keep)\n",
    "X_1_val = extend_df(X_1_val, cols_to_keep)\n",
    "X_2_train = extend_df(X_2_train, cols_to_keep)\n",
    "X_2_val = extend_df(X_2_val, cols_to_keep)\n",
    "\n",
    "\n",
    "# standardize data\n",
    "\n",
    "# Standardize X_1 data\n",
    "scaler_1 = StandardScaler()\n",
    "X_1_train = scaler_1.fit_transform(X_1_train)\n",
    "X_1_val = scaler_1.transform(X_1_val)\n",
    "\n",
    "# Standardize X_2 data\n",
    "scaler_2 = StandardScaler()\n",
    "X_2_train = scaler_2.fit_transform(X_2_train)\n",
    "X_2_val = scaler_2.transform(X_2_val)\n",
    "\n",
    "\n",
    "# stack data\n",
    "\n",
    "# Stack the training data and validation data for X and y\n",
    "X_train = np.vstack([X_1_train, X_2_train])\n",
    "X_val = np.vstack([X_1_val, X_2_val])\n",
    "\n",
    "# Combine y data for training and validation\n",
    "y_train = pd.concat([y_1_train, y_2_train], ignore_index=True)\n",
    "y_val = pd.concat([y_1_val, y_2_val], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/ml4g-env-2/lib/python3.12/site-packages/lightgbm/engine.py:204: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000311 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1352\n",
      "[LightGBM] [Info] Number of data points in the train set: 28620, number of used features: 11\n",
      "[LightGBM] [Info] Start training from score 50.915895\n",
      "Spearman correlation on validation set: 0.7671909991090514\n"
     ]
    }
   ],
   "source": [
    "# Prepare the LightGBM dataset\n",
    "train_data = lgb.Dataset(X_train, label=y_train)\n",
    "val_data = lgb.Dataset(X_val, label=y_val, reference=train_data)\n",
    "\n",
    "# Parameters for LightGBM with Huber loss\n",
    "params = {\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'huber',\n",
    "    'alpha': 0.9,\n",
    "    'learning_rate': 0.06,\n",
    "    'num_leaves': 40,\n",
    "    'n_estimators': 100\n",
    "}\n",
    "\n",
    "# Train the model\n",
    "model = lgb.train(\n",
    "    params,\n",
    "    train_data,\n",
    "    valid_sets=[train_data, val_data],\n",
    "    valid_names=['train', 'val'],\n",
    ")\n",
    "\n",
    "# Predict on validation set\n",
    "y_pred_val = model.predict(X_val)\n",
    "\n",
    "# Calculate Spearman correlation for the predictions on validation set\n",
    "spearman_corr, _ = spearmanr(y_val, y_pred_val)\n",
    "print(f'Spearman correlation on validation set: {spearman_corr}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train final model with all data (train and valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add zipping as they suggested in the end !!!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml4g-proj-1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
