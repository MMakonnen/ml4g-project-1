{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "THINGS TO KEEP IN MIND / IMPROVE UPON:\n",
    "\n",
    "major:\n",
    "- for now just stack the data of the two cell lines and train model using combined data with the hope of fitting a model that generalizes beyond cell line\n",
    "-> just stacking the data might not be great\n",
    "- think about trying signal processing tricks to extend features using ML4H libray, on top of self designed features\n",
    "- test simple implementation of CNN (look int literature for what makes sense, see lectures for some general steps) + on top use surrogate spearman correlation loss to train the CNN\n",
    "-> make sure to not give CNN too many par\n",
    "- try getting pairwise loss surrogate for spearman corr to work (but prob have to then go with pytorch), however is approx only and can get quite comp expensive for large datasets due to pairwise comparison\n",
    "- right now use simple setup and dont use all the data that we could be using\n",
    "-> once have det optimal model with optimal hyperpar train on all train & val data and use for final prediction\n",
    "\n",
    "minor:\n",
    "- dropped the gene names, so double check if the gene ordering aligns in the x and y data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.stats import spearmanr\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions used\n",
    "\n",
    "\n",
    "# extends cell line data with simple features\n",
    "def extend_df(df, col_to_keep):\n",
    "    # Add a binary column for strand\n",
    "    df['strand_binary'] = df['strand'].map({'+': 1, '-': 0})\n",
    "    \n",
    "    # gene length\n",
    "    df['gene_length'] = df['gene_end'] - df['gene_start']\n",
    "    \n",
    "    # transcription site length\n",
    "    df['trans_site_len'] = df['TSS_end'] - df['TSS_start']\n",
    "    \n",
    "    # ratio transcription site length & land gene length\n",
    "    df['trans_gene_ratio'] = df['trans_site_len'] / df['gene_length']\n",
    "    \n",
    "    return df[col_to_keep]\n",
    "\n",
    "\n",
    "\n",
    "# NOT BEING USED !!!!\n",
    "# Custom surrogate loss function for Spearman correlation (Pairwise loss)\n",
    "def pairwise_loss(y_true, y_pred):\n",
    "    # Convert predicted values to a 1D numpy array\n",
    "    y_pred = y_pred.ravel()\n",
    "    \n",
    "    # Calculate the pairwise differences for predicted values\n",
    "    pred_diff = np.expand_dims(y_pred, axis=1) - np.expand_dims(y_pred, axis=0)\n",
    "    \n",
    "    # Calculate the pairwise differences for true values\n",
    "    true_diff = np.expand_dims(y_true, axis=1) - np.expand_dims(y_true, axis=0)\n",
    "    \n",
    "    # Sigmoid of the differences (this creates a smooth approximation for ranking)\n",
    "    sigmoid_pred_diff = 1 / (1 + np.exp(-pred_diff))\n",
    "    \n",
    "    # Compute the gradient as the difference between the predictions and the true order\n",
    "    grad = sigmoid_pred_diff - (true_diff > 0).astype(np.float32)\n",
    "    \n",
    "    # The Hessian is a constant in pairwise loss (can be set to a fixed value)\n",
    "    hess = np.ones_like(grad)\n",
    "    \n",
    "    return grad.ravel(), hess.ravel()\n",
    "\n",
    "\n",
    "# Custom evaluation metric for Spearman correlation\n",
    "# -> used as eval metric during training ()\n",
    "def spearman_correlation(preds, train_data):\n",
    "    y_true = train_data.get_label()\n",
    "    corr, _ = spearmanr(y_true, preds)\n",
    "    return 'spearman', corr, True\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "\n",
    "\n",
    "# reproducibility seed\n",
    "seed = 42\n",
    "\n",
    "hpo_size = 0.2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' \\n1) To find the optimal model and hyperparameters we use the train data (split for training and hyperparoptim)\\n2) Once we have found the optimal model we get an estimate for the test set error on the validation data\\n3) Before submission we train the best model (model + hyperpar) on the train and validation data and then predict test for submission\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load data\n",
    "\n",
    "\n",
    "# load cell line data (x/y for feature/target, 1/2/3 for cell line and train/val/test indicating assignment)\n",
    "\n",
    "# features \n",
    "x_1_train = pd.read_csv('data/CAGE-train/X1_train_info.tsv', sep='\\t')\n",
    "x_1_val = pd.read_csv('data/CAGE-train/X1_val_info.tsv', sep='\\t')\n",
    "x_2_train = pd.read_csv('data/CAGE-train/X2_train_info.tsv', sep='\\t')\n",
    "x_2_val = pd.read_csv('data/CAGE-train/X2_val_info.tsv', sep='\\t')\n",
    "\n",
    "x_3_test = pd.read_csv('data/CAGE-train/X3_test_info.tsv', sep='\\t')\n",
    "\n",
    "# targets\n",
    "y_1_train = pd.read_csv('data/CAGE-train/X1_train_y.tsv', sep='\\t')\n",
    "y_1_val = pd.read_csv('data/CAGE-train/X1_val_y.tsv', sep='\\t')\n",
    "y_2_train = pd.read_csv('data/CAGE-train/X2_train_y.tsv', sep='\\t')\n",
    "y_2_val = pd.read_csv('data/CAGE-train/X2_val_y.tsv', sep='\\t')\n",
    "\n",
    "\n",
    "# save gene order (might be relevant down the line)\n",
    "genes_1_train = x_1_train['gene_name']\n",
    "genes_1_val = x_1_val['gene_name']\n",
    "genes_2_train = x_2_train['gene_name']\n",
    "genes_2_val = x_2_val['gene_name']\n",
    "genes_3_test = x_3_test['gene_name']\n",
    "\n",
    "\n",
    "# features to use for prediction\n",
    "pred_feat = ['strand_binary', 'gene_length', 'trans_site_len', 'trans_gene_ratio']\n",
    "\n",
    "''' \n",
    "1) To find the optimal model and hyperparameters we use the train data (split for training and hyperparoptim)\n",
    "2) Once we have found the optimal model we get an estimate for the test set error on the validation data\n",
    "3) Before submission we train the best model (model + hyperpar) on the train and validation data and then predict test for submission\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data preprocessing & feature engineering\n",
    "\n",
    "# ...\n",
    "x_1_train = extend_df(x_1_train, pred_feat)\n",
    "x_1_val = extend_df(x_1_val, pred_feat)\n",
    "x_2_train = extend_df(x_2_train, pred_feat)\n",
    "x_2_val = extend_df(x_2_val, pred_feat)\n",
    "\n",
    "x_3_test = extend_df(x_3_test, pred_feat)\n",
    "\n",
    "\n",
    "# drop gene name from y\n",
    "y_1_train = y_1_train.drop('gene_name', axis=1)\n",
    "y_1_val = y_1_val.drop('gene_name', axis=1)\n",
    "y_2_train = y_2_train.drop('gene_name', axis=1)\n",
    "y_2_val = y_2_val.drop('gene_name', axis=1)\n",
    "\n",
    "\n",
    "# stack features of cell line 1 and 2 (STARTING POINT, CAN TEST SOMETHING ELSE TO IMPROVE UPON THIS)\n",
    "# CAREFUL, remember stacking order for gene names if necessary!!!\n",
    "\n",
    "# x\n",
    "x_1_2_train = pd.concat([x_1_train, x_2_train], ignore_index=True)\n",
    "x_1_2_val = pd.concat([x_1_val, x_2_val], ignore_index=True)\n",
    "# y\n",
    "y_1_2_train = pd.concat([y_1_train, y_2_train], ignore_index=True)\n",
    "y_1_2_val = pd.concat([y_1_val, y_2_val], ignore_index=True)\n",
    "\n",
    "\n",
    "# split train data into data for training and hyperparoptim\n",
    "x_1_2_train_only, x_1_2_train_hpo, y_1_2_train_only, y_1_2_train_hpo = train_test_split(x_1_2_train, y_1_2_train, test_size=hpo_size, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mikael/anaconda3/envs/ml4g-proj-1/lib/python3.11/site-packages/lightgbm/engine.py:204: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000422 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 558\n",
      "[LightGBM] [Info] Number of data points in the train set: 22896, number of used features: 4\n",
      "[LightGBM] [Info] Start training from score 51.197179\n",
      "Spearman correlation on validation set (x_y_1_2_train_hpo): 0.15443240578736467\n"
     ]
    }
   ],
   "source": [
    "# Data to train LightGBM model\n",
    "x_y_1_2_train_only = lgb.Dataset(x_1_2_train_only, label=y_1_2_train_only)\n",
    "\n",
    "# Parameters for LightGBM with Huber loss\n",
    "params = {\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'huber',\n",
    "    'alpha': 0.9,\n",
    "    'learning_rate': 0.05,\n",
    "    'num_leaves': 40,\n",
    "    'n_estimators': 100\n",
    "}\n",
    "\n",
    "# train model\n",
    "model = lgb.train(\n",
    "    params,\n",
    "    x_y_1_2_train_only,\n",
    ")\n",
    "\n",
    "# eval model for hyperparoptim\n",
    "y_pred_hpo = model.predict(x_1_2_train_hpo)\n",
    "\n",
    "# Calculate Spearman correlation for the predictions\n",
    "spearman_corr, _ = spearmanr(y_1_2_train_hpo, y_pred_hpo)\n",
    "print(f'Spearman correlation on validation set (x_y_1_2_train_hpo): {spearman_corr}')\n",
    "# -> use this for hyperparoptim\n",
    "\n",
    "# FINAL ESTIMATE TEST SET ERROR ... (use val data, but in end use test data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Work Package 1.1 - Modeling Choices & Data Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: \n",
    "# Load your feature (bed and/or bigwig and/or fasta) and target files (tsv) here.\n",
    "# Decide which features to use for training. Feel free to process them however you need.\n",
    "\n",
    "# NOTE: \n",
    "# bed and bigwig files contain signals of all chromosomes (including sex chromosomes).\n",
    "# Training and validation split based on chromosomes has been done for you. \n",
    "# However, you can resplit the data in any way you want.\n",
    "\n",
    "path_data = \"/path/to/your/data/files\"  # TODO\n",
    "path_test = \"/path/to/test/info/file\"   # X3_test_info.tsv ; TODO\n",
    "test_genes = pd.read_csv(path_test, sep='\\t')\n",
    "# ---------------------------INSERT CODE HERE---------------------------\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Work Package 1.2 - Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: \n",
    "# Select the best model to predict gene expression from the obtained features in WP 1.1.\n",
    "\n",
    "# ---------------------------INSERT CODE HERE---------------------------\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Work Package 1.3 - Prediction on Test Data (Evaluation Metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:\n",
    "# Using the model trained in WP 1.2, make predictions on the test data (chr 1 of cell line X3).\n",
    "# Store predictions in a variable called \"pred\" which is a numpy array.\n",
    "\n",
    "pred = None\n",
    "# ---------------------------INSERT CODE HERE---------------------------\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "\n",
    "# Check if \"pred\" meets the specified constrains\n",
    "assert isinstance(pred, np.ndarray), 'Prediction array must be a numpy array'\n",
    "assert np.issubdtype(pred.dtype, np.number), 'Prediction array must be numeric'\n",
    "assert pred.shape[0] == len(test_genes), 'Each gene should have a unique predicted expression'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Store Predictions in the Required Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store predictions in a ZIP. \n",
    "# Upload this zip on the project website under \"Your submission\".\n",
    "# Zip this notebook along with the conda environment (and README, optional) and upload this under \"Your code\".\n",
    "\n",
    "save_dir = 'path/to/save/output/file'  # TODO\n",
    "file_name = 'gex_predicted.csv'         # PLEASE DO NOT CHANGE THIS\n",
    "zip_name = \"LastName_FirstName_Project1.zip\" # TODO\n",
    "save_path = f'{save_dir}/{zip_name}'\n",
    "compression_options = dict(method=\"zip\", archive_name=file_name)\n",
    "\n",
    "test_genes['gex_predicted'] = pred.tolist()\n",
    "test_genes[['gene_name', 'gex_predicted']].to_csv(save_path, compression=compression_options)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml4g-proj-1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
