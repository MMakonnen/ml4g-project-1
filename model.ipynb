{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: the feature engineering is performed via functions in gen.py (see scripts). To create the feature engineered datasets that we use in this script just run the bash.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import lightgbm as lgb\n",
    "from scipy.stats import spearmanr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions used\n",
    "\n",
    "\n",
    "# extends data with simple features:\n",
    "# > convert strand from + - to 1 0\n",
    "# > gene length\n",
    "# > transcription site length\n",
    "# > ratio of transciption site length to gene length\n",
    "def extend_df(df, cols_to_keep):\n",
    "    # Add a binary column for strand\n",
    "    df['strand_binary'] = df['strand'].map({'+': 1, '-': 0})\n",
    "    \n",
    "    # gene length\n",
    "    df['gene_length'] = df['gene_end'] - df['gene_start']\n",
    "    \n",
    "    # transcription site length\n",
    "    df['trans_site_len'] = df['TSS_end'] - df['TSS_start']\n",
    "    \n",
    "    # ratio transcription site length & land gene length\n",
    "    df['trans_gene_ratio'] = df['trans_site_len'] / df['gene_length']\n",
    "\n",
    "    new_cols = ['strand_binary', 'gene_length', 'trans_site_len', 'trans_gene_ratio']\n",
    "\n",
    "    \n",
    "    return df[cols_to_keep + new_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load already feature engineered data\n",
    "\n",
    "\n",
    "# Paths\n",
    "X_1_train_path = 'data/X1-train/features.tsv'\n",
    "y_1_train_path = 'data/X1-train/y.tsv'\n",
    "\n",
    "X_1_val_path = 'data/X1-val/features.tsv'\n",
    "y_1_val_path = 'data/X1-val/y.tsv'\n",
    "\n",
    "X_2_train_path = 'data/X2-train/features.tsv'\n",
    "y_2_train_path = 'data/X2-train/y.tsv'\n",
    "\n",
    "X_2_val_path = 'data/X2-val/features.tsv'\n",
    "y_2_val_path = 'data/X2-val/y.tsv'\n",
    "\n",
    "# Load data\n",
    "X_1_train = pd.read_csv(X_1_train_path, sep='\\t')\n",
    "y_1_train = pd.read_csv(y_1_train_path, sep='\\t')\n",
    "\n",
    "X_1_val = pd.read_csv(X_1_val_path, sep='\\t')\n",
    "y_1_val = pd.read_csv(y_1_val_path, sep='\\t')\n",
    "\n",
    "X_2_train = pd.read_csv(X_2_train_path, sep='\\t')\n",
    "y_2_train = pd.read_csv(y_2_train_path, sep='\\t')\n",
    "\n",
    "X_2_val = pd.read_csv(X_2_val_path, sep='\\t')\n",
    "y_2_val = pd.read_csv(y_2_val_path, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some further data preprocessing\n",
    "\n",
    "\n",
    "cols_to_keep = ['DNase_num_peaks', 'DNase_avg_peaks', 'H3K4me1_num_peaks', 'H3K4me1_avg_peaks', 'H3K4me3_num_peaks', \n",
    "                'H3K4me3_avg_peaks', 'H3K27ac_num_peaks', 'H3K27ac_avg_peaks']\n",
    "\n",
    "# cols_to_keep = ['dnase_val', 'dnase_dist', 'H3K4me3_signal', 'H3K4me3_distance', 'H3K4me3_num_peaks', \n",
    "#                 'H3K4me3_avg_peaks', 'H3K27ac_signal', 'H3K27ac_distance', 'H3K27ac_num_peaks', \n",
    "#                 'H3K27ac_avg_peaks']\n",
    "\n",
    "\n",
    "# extend data by some basic transcription site features\n",
    "X_1_train = extend_df(X_1_train, cols_to_keep)\n",
    "X_1_val = extend_df(X_1_val, cols_to_keep)\n",
    "X_2_train = extend_df(X_2_train, cols_to_keep)\n",
    "X_2_val = extend_df(X_2_val, cols_to_keep)\n",
    "\n",
    "\n",
    "# standardize data\n",
    "\n",
    "# Standardize X_1 data\n",
    "scaler_1 = StandardScaler()\n",
    "X_1_train = scaler_1.fit_transform(X_1_train)\n",
    "X_1_val = scaler_1.transform(X_1_val)\n",
    "\n",
    "# Standardize X_2 data\n",
    "scaler_2 = StandardScaler()\n",
    "X_2_train = scaler_2.fit_transform(X_2_train)\n",
    "X_2_val = scaler_2.transform(X_2_val)\n",
    "\n",
    "\n",
    "# stack data\n",
    "\n",
    "# Stack the training data and validation data for X and y\n",
    "X_train = np.vstack([X_1_train, X_2_train])\n",
    "X_val = np.vstack([X_1_val, X_2_val])\n",
    "\n",
    "# Combine y data for training and validation\n",
    "y_train = pd.concat([y_1_train, y_2_train], ignore_index=True)\n",
    "y_val = pd.concat([y_1_val, y_2_val], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mikael/anaconda3/envs/ml4g-proj-1/lib/python3.11/site-packages/lightgbm/engine.py:204: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000695 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1588\n",
      "[LightGBM] [Info] Number of data points in the train set: 28620, number of used features: 12\n",
      "[LightGBM] [Info] Start training from score 50.915895\n",
      "Spearman correlation on validation set: 0.7871156138306246\n"
     ]
    }
   ],
   "source": [
    "# Prepare the LightGBM dataset\n",
    "train_data = lgb.Dataset(X_train, label=y_train)\n",
    "val_data = lgb.Dataset(X_val, label=y_val, reference=train_data)\n",
    "\n",
    "# Parameters for LightGBM with Huber loss\n",
    "params = {\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'huber',\n",
    "    'alpha': 0.9,\n",
    "    'learning_rate': 0.5,\n",
    "    'num_leaves': 50,\n",
    "    'n_estimators': 100\n",
    "}\n",
    "\n",
    "# Train the model\n",
    "model = lgb.train(\n",
    "    params,\n",
    "    train_data,\n",
    "    valid_sets=[train_data, val_data],\n",
    "    valid_names=['train', 'val'],\n",
    ")\n",
    "\n",
    "# Predict on validation set\n",
    "y_pred_val = model.predict(X_val)\n",
    "\n",
    "# Calculate Spearman correlation for the predictions on validation set\n",
    "spearman_corr, _ = spearmanr(y_val, y_pred_val)\n",
    "print(f'Spearman correlation on validation set: {spearman_corr}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mikael/anaconda3/envs/ml4g-proj-1/lib/python3.11/site-packages/lightgbm/engine.py:204: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011796 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1588\n",
      "[LightGBM] [Info] Number of data points in the train set: 32568, number of used features: 12\n",
      "[LightGBM] [Info] Start training from score 52.361132\n",
      "Final model trained on the combined dataset for deployment.\n"
     ]
    }
   ],
   "source": [
    "# --- Final Model Training for Deployment (/ real test data prediction) ---\n",
    "# Combine the training and validation data for final training\n",
    "X_final_train = np.vstack([X_train, X_val])\n",
    "y_final_train = pd.concat([y_train, y_val], ignore_index=True)\n",
    "\n",
    "# Prepare the LightGBM dataset for the combined data\n",
    "final_train_data = lgb.Dataset(X_final_train, label=y_final_train)\n",
    "\n",
    "# Train the final model on the entire combined dataset with the optimal parameters\n",
    "final_model = lgb.train(\n",
    "    params,\n",
    "    final_train_data,\n",
    "    valid_sets=[final_train_data],\n",
    "    valid_names=['train'],\n",
    ")\n",
    "\n",
    "print(\"Final model trained on the combined dataset for deployment.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load initial test data for initial gene name order (before feature engineering)\n",
    "original_test = pd.read_csv('data/CAGE-train/X3_test_info.tsv', sep='\\t')\n",
    "gene_names_orig_order = original_test['gene_name']\n",
    "\n",
    "\n",
    "# load feature engineered test data (with different order of rows)\n",
    "X_test_path = 'data/X3-test/features.tsv'\n",
    "X_test = pd.read_csv(X_test_path, sep='\\t')\n",
    "\n",
    "# gene names (in changed order)\n",
    "gene_names_test_feat_eng = X_test['gene_name']\n",
    "\n",
    "# extend data by some basic transcription site features\n",
    "X_test = extend_df(X_test, cols_to_keep)\n",
    "\n",
    "# standardize data\n",
    "scaler_test = StandardScaler()\n",
    "X_test = scaler_test.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 8.65539254, 12.89496183,  8.65539254, ..., 41.27389379,\n",
       "        8.91241666, 29.23367441])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# FINAL GENE EXPRESSION PREDICTION FOR TEST DATA\n",
    "pred = final_model.predict(X_test)\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate gene names and predictions for feature engineered features (in wrong gene order)\n",
    "final_pred_unsorted = pd.concat([gene_names_test_feat_eng, pd.Series(pred)], ignore_index=True, axis=1)\n",
    "\n",
    "# Rename the columns\n",
    "final_pred_unsorted.columns = ['gene_name', 'gex_predicted']\n",
    "\n",
    "# sort the predicted genes according to intially order for test data\n",
    "final_pred_sorted = final_pred_unsorted.set_index('gene_name').loc[gene_names_orig_order].reset_index()\n",
    "\n",
    "test_genes = final_pred_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store predictions in a ZIP. \n",
    "# Upload this zip on the project website under \"Your submission\".\n",
    "# Zip this notebook along with the conda environment (and README, optional) and upload this under \"Your code\".\n",
    "\n",
    "save_dir = './submission_res'  # TODO\n",
    "file_name = 'gex_predicted.csv'         # PLEASE DO NOT CHANGE THIS\n",
    "zip_name = \"Makonnen_Mikael_Project1.zip\" # TODO\n",
    "save_path = f'{save_dir}/{zip_name}'\n",
    "compression_options = dict(method=\"zip\", archive_name=file_name)\n",
    "\n",
    "#test_genes['gex_predicted'] = pred.tolist()\n",
    "test_genes[['gene_name', 'gex_predicted']].to_csv(save_path, compression=compression_options)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml4g-env-2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
